{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/physicsllama/instrument-detector/blob/main/music.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary remarks and ideas\n"
      ],
      "metadata": {
        "id": "4QPC9ZjDZ9XO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjK9IDs4-nVX"
      },
      "outputs": [],
      "source": [
        "# Goal: AI that recognizes similarity between songs. It'll give a list of similar songs to one.\n",
        "\n",
        "# Maybe a good first thing is to do a clustering thing, where it just categorizes by genre.\n",
        "\n",
        "#Maybe better goal: shazam but for recognizing songs from singing of a melody (NLR useful?)\n",
        "# Try supervised ML, but also unsupervised.\n",
        "\n",
        "# other thing: use technique from MIT course to swipe thru song?\n",
        "\n",
        "#use clustering algorithm to distinguish different instruments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0nz9bP6K7Lk"
      },
      "outputs": [],
      "source": [
        "# First need a database with songs. Start out with something simple, and short audio things\n",
        "# Then need to process data. Will I need to Fourier transform/something like that, or can I just feed it into the network?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "L0hpzUDkEFK6",
        "outputId": "7085325c-4214-4430-ac05-8197ed3bf2ee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nUseful: https://www.tensorflow.org/io/tutorials/audio\\n\\nhttps://www.altexsoft.com/blog/audio-analysis/\\n\\n\\n\\n\\nAudio sources: \\nhttps://freesound.org/browse/packs/?order=-last_updated\\n\\nhttps://bigsoundbank.com/search\\n\\nhttps://research.google.com/audioset/index.html\\n\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' Some beginning things:\n",
        "\n",
        "1) Take down data from wav files. Use files containing a single note, played with a pitch fork\n",
        "and also played with different instruments. Compare the data. Take Fourier transform. Try\n",
        "to find what note it is from this. Try to deduce instrument playing the note! Normalize stuff to have same volume, same length of audio, etc\n",
        "\n",
        "samples: notes from different instruments https://philharmonia.co.uk/resources/sound-samples/\n",
        "\n",
        "2) Take down data from more complicated files (e.g., 5 seconds).\n",
        "\n",
        "3) Get a sense of how big a neural network I need to represent data from a song\n",
        "(how many inputs are there?)\n",
        "\n",
        "4) Process data for set of songs from different genres (e.g. 3 each from classical,\n",
        "hard rock, pop, and rap) \n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "Useful: https://www.tensorflow.org/io/tutorials/audio\n",
        "\n",
        "https://www.altexsoft.com/blog/audio-analysis/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Audio sources: \n",
        "https://freesound.org/browse/packs/?order=-last_updated\n",
        "\n",
        "https://bigsoundbank.com/search\n",
        "\n",
        "https://research.google.com/audioset/index.html\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6FFMDiJWN9u"
      },
      "source": [
        "Take data and separate into training/testing, and arrange into folders in convenient way. Do both myself manually in Python and with automatic pytorch/whatever thing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QrFWxopBx9G"
      },
      "source": [
        "Make sure all audio files same length. Perhaps use only files with normal notes (no trill/tremolo nonsense?) first? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUMDrNwGCAMU"
      },
      "source": [
        "Sine or something for activation functions? Also, pre-process with Fourier? Or do conv neural net? Some Fourier concept in there.\n",
        "\n",
        "Standard way is to turn into images. Can I come up with something better? [Why are conv nets good for images? What is a similar \"goodness\" thing we can exploit for the properties of audio? Study conv nets first!]\n",
        "\n",
        "Now we begin a standard way of analyzing audio in ML (turning clips into spectrograms). I might experiment with other ways later - Shazam seems to have a more clever way to do it, and I can try to find my own methods.\n",
        "\n",
        "One possibility for instrument detection: maybe it's enough to just take the FFT of a 0.5 or 1 second clip, and the spectral decomposition is enough of a fingerprint?\n",
        "Then turning this fingerprint into a filter might help us isolate individual instruments in songs!\n",
        "\n",
        "Along the previous idea: maybe ML thing can be trained on each individual song (after some pre-training?) to separate out its instruments on a case-by-case basis. It takes e.g., a 1 second clip and within that clip tries to identify different fingerprints left by instruments.\n",
        "\n",
        "Maybe use something with statistical correlations or autocorrelation or something?\n",
        "\n",
        "OTHER IDEA: make *my own* data set where I use e.g., Logic & a keyboard to make songs. The input will be full songs, the output will be the separated instrument tracks (which I'll have since I made the songs on Logic). Try actual songs and things like that, also just nonsense songs. Many instruments, few instruments, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing dependencies"
      ],
      "metadata": {
        "id": "PLGuCj4oaCxO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXwgXz-FUkgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70aa64d7-db47-4bcb-d5e4-c7a12645e169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-io\n",
            "  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 69.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.26.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (0.26.0)\n",
            "Installing collected packages: tensorflow-io\n",
            "Successfully installed tensorflow-io-0.26.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install tensorflow-io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FriCjs2pWJXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2fe433-8248-4d14-a8bb-bce881165ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.9.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.9.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.26.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (21.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.47.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.7/dist-packages (0.26.0)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n",
        "#!pip uninstall tensorflow-io\n",
        "!pip install tensorflow-gpu\n",
        "!pip install --no-deps tensorflow-io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CN1rGWdIWcSg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9t632vGVMX_",
        "outputId": "42101c94-4f0d-498b-afc0-b4ce35470fb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Songs',\n",
              " 'Instruments',\n",
              " 'train_instruments',\n",
              " '.ipynb_checkpoints',\n",
              " 'testing_instruments',\n",
              " 'test.wav',\n",
              " 'Music.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "#drive.mount('/content/drive')\n",
        "mainpath = \"/content/drive/MyDrive/Music/\"\n",
        "os.chdir(mainpath)\n",
        "os.getcwd()\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zboon_yYGtY"
      },
      "source": [
        "# Organizing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuoWVN2GU-nF"
      },
      "outputs": [],
      "source": [
        "instrumentpath = \"Instruments/all-samples/\"\n",
        "os.chdir(instrumentpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5xWR5_sb-Sy"
      },
      "outputs": [],
      "source": [
        "instrument_labels = os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GRvU9NicCm4"
      },
      "outputs": [],
      "source": [
        "os.chdir('./')\n",
        "os.chdir(mainpath)\n",
        "os.chdir('Instruments/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZQYRoFshhhh",
        "outputId": "354b6aa5-6cc9-4d5d-bfd3-f01fe12aca78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Instruments_train', 'Instruments_test', 'all-samples']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUPbcM70k71P",
        "outputId": "dc957e6b-692d-4b57-99c1-b56aa8d77861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cello',\n",
              " 'trombone',\n",
              " 'bassoon',\n",
              " 'violin',\n",
              " 'bass clarinet',\n",
              " 'banjo',\n",
              " 'clarinet',\n",
              " 'trumpet',\n",
              " 'oboe',\n",
              " 'guitar',\n",
              " 'contrabassoon',\n",
              " 'flute',\n",
              " 'viola',\n",
              " 'mandolin',\n",
              " 'double bass',\n",
              " 'tuba',\n",
              " 'cor anglais',\n",
              " 'french horn',\n",
              " 'percussion',\n",
              " 'saxophone']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instrument_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9aoMak3t6xg",
        "outputId": "30cfa63c-b4ef-44cf-d72b-11a0034f93ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moved stuff from cello!\n",
            "Moved stuff from trombone!\n",
            "Moved stuff from bassoon!\n",
            "Moved stuff from violin!\n",
            "Moved stuff from bass clarinet!\n",
            "Moved stuff from banjo!\n",
            "Moved stuff from clarinet!\n",
            "Moved stuff from trumpet!\n",
            "Moved stuff from oboe!\n",
            "Moved stuff from guitar!\n",
            "Moved stuff from contrabassoon!\n",
            "Moved stuff from flute!\n",
            "Moved stuff from viola!\n",
            "Moved stuff from mandolin!\n",
            "Moved stuff from double bass!\n",
            "Moved stuff from tuba!\n",
            "Moved stuff from cor anglais!\n",
            "Moved stuff from french horn!\n",
            "Moved stuff from percussion!\n",
            "Moved stuff from saxophone!\n"
          ]
        }
      ],
      "source": [
        "os.chdir('./')\n",
        "os.chdir(mainpath)\n",
        "os.chdir('Instruments/')\n",
        "\n",
        "for instrument in instrument_labels:\n",
        "  sounds = os.listdir('all-samples/' + instrument)\n",
        "  for sound in sounds:\n",
        "    if sound == '.DS_Store':\n",
        "      pass\n",
        "    else:\n",
        "      soundpath1 = 'all-samples/' + instrument + '/' + sound\n",
        "      p = np.random.rand()\n",
        "      if p <= 0.3:\n",
        "        soundpath2 = 'Instruments_test/' + sound\n",
        "      else:\n",
        "        soundpath2 = 'Instruments_train/' + sound\n",
        "      os.rename(soundpath1, soundpath2)\n",
        "  print('Moved stuff from ' + instrument + '!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r96LemMcYLlp",
        "outputId": "27c0b501-4ce5-4046-bcda-43a1a2f25c69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Instruments_train', 'Instruments_test', 'all-samples']"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-Ey4zUOs6Ke"
      },
      "outputs": [],
      "source": [
        "os.chdir('Instruments_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBb1_FDXoR41",
        "outputId": "31fb52a9-34c1-4696-bb8b-819544f4caff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4076"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prkupPjYoUke"
      },
      "outputs": [],
      "source": [
        "os.chdir('../')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARhB-76OoYh_"
      },
      "outputs": [],
      "source": [
        "os.chdir('Instruments_train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QHlV47Toa76",
        "outputId": "9724f280-b596-4a90-8a64-daf9432c27ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3QFY5eZocPe"
      },
      "outputs": [],
      "source": [
        "os.chdir('../')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A7gfHE1ot39"
      },
      "outputs": [],
      "source": [
        "#next: delete random folders of percussion and stuff like that that were left over in training and testing. To do this, check that all files in the\n",
        "#training and testing folders were "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing data"
      ],
      "metadata": {
        "id": "tfQUpgmDT8FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that will make things easier:\n",
        "def apply_to_set(f, old_set):\n",
        "  new_set = []\n",
        "  for i in old_set:\n",
        "    new_set.append(np.array(f(i)))\n",
        "  return new_set"
      ],
      "metadata": {
        "id": "_rrZpUGEEqCc"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_samples = os.listdir('Instruments/Instruments_test/')"
      ],
      "metadata": {
        "id": "PDKZLeb15jIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalnotes_testing = []\n",
        "for i in testing_samples:\n",
        "  if i.find('normal')!=-1:\n",
        "    normalnotes_testing.append(i)\n",
        "np.save('testing_instruments/testing_labels', normalnotes_testing, allow_pickle='True')"
      ],
      "metadata": {
        "id": "zY_crzhm7SNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_samples = os.listdir('Instruments/Instruments_train/')"
      ],
      "metadata": {
        "id": "KnABZpB4DTL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalnotes_training = []\n",
        "for i in training_samples:\n",
        "  if i.find('normal')!=-1:\n",
        "    normalnotes_training.append(i)\n",
        "np.save('train_instruments/training_labels', normalnotes_training, allow_pickle='True')"
      ],
      "metadata": {
        "id": "HV_U2z5VDO_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make functions nicer later! (add domain and range, as well as function descriptions)\n",
        "def io_to_array(io_tensor):\n",
        "  trimmed_tensor = io_tensor[10:]\n",
        "  squeezed_tensor = tf.squeeze(trimmed_tensor)\n",
        "  return np.array(squeezed_tensor)"
      ],
      "metadata": {
        "id": "wgp0u91rQEob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('testing_instruments')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ZZnpZ-FZBR",
        "outputId": "2aa6b674-6605-4bdf-a23e-97e2685c205a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['testing_labels.npy', 'normal_arrays_test.npy']"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_arrays = []\n",
        "for i in range(len(normalnotes_testing)):\n",
        "  path = 'Instruments/Instruments_test/' + normalnotes_testing[i]\n",
        "  iofile = tfio.audio.AudioIOTensor(path)\n",
        "  testing_arrays.append(io_to_array(iofile))\n",
        "  if i %100 == 0:\n",
        "    print (str(i) + ' done!')"
      ],
      "metadata": {
        "id": "u04vHORWXnWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('testing_instruments/testing_arrays', testing_arrays, allow_pickle='True')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm9OVyH5FVHJ",
        "outputId": "36935f57-558e-4c87-c39e-1bd34e77ade5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_arrays = []\n",
        "for i in range(len(normalnotes_training)):\n",
        "  path = 'Instruments/Instruments_train/' + normalnotes_training[i]\n",
        "  iofile = tfio.audio.AudioIOTensor(path)\n",
        "  training_arrays.append(io_to_array(iofile))\n",
        "  if i %100 == 0:\n",
        "    print (str(i) + ' done!')"
      ],
      "metadata": {
        "id": "Q84BhyVMFSTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('train_instruments/training_arrays', training_arrays, allow_pickle='True')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7Mp1GJIF2YW",
        "outputId": "5e170f8e-9ff6-4882-bd36-fc235f747255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata and selecting useful data"
      ],
      "metadata": {
        "id": "cEHxQvctSuJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Below I first understand the data set. How many of each type of instrument in training? In testing?\n",
        "# What is the typical audio length for each instrument?"
      ],
      "metadata": {
        "id": "D1bDyXk4TSzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to load a data file: np.load('train_instruments/normal_arrays.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "PNZA1z_wk3nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arrays = np.load('train_instruments/training_arrays.npy', allow_pickle=True)\n",
        "training_labels = np.load('train_instruments/training_labels.npy', allow_pickle = True)\n",
        "testing_arrays = np.load('testing_instruments/testing_arrays.npy', allow_pickle=True)\n",
        "testing_labels = np.load('testing_instruments/testing_labels.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "VlHnhFu0kgaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instrument_labels = ['cello',\n",
        " 'trombone',\n",
        " 'bassoon',\n",
        " 'violin',\n",
        " 'bass-clarinet',\n",
        " 'banjo',\n",
        " 'clarinet',\n",
        " 'trumpet',\n",
        " 'oboe',\n",
        " 'guitar',\n",
        " 'contrabassoon',\n",
        " 'flute',\n",
        " 'viola',\n",
        " 'mandolin',\n",
        " 'double-bass',\n",
        " 'tuba',\n",
        " 'french-horn',\n",
        " 'saxophone',\n",
        " 'english-horn']"
      ],
      "metadata": {
        "id": "Brscu6o6TMtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First number in value of dictionary metadata is number of times the instrument is included in the data.\n",
        "# Second number in value of dictionary is average duration of a clip containing the instrument.\n",
        "def files_to_metadata(data, labels):\n",
        "  metadata = {}\n",
        "  for i in instrument_labels:\n",
        "    metadata[i] = [0, 0]\n",
        "  \n",
        "  for i in range(len(labels)):\n",
        "    for j in instrument_labels:\n",
        "      if labels[i].find(j) == 0:\n",
        "        metadata[j][0] += 1\n",
        "        metadata[j][1] += len(data[i])\n",
        "\n",
        "  for i in instrument_labels:\n",
        "    assert metadata[i][0] != 0\n",
        "    metadata[i][1] = int(metadata[i][1]/metadata[i][0])\n",
        "  return metadata"
      ],
      "metadata": {
        "id": "QZg_WTI-qbtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_metadata = files_to_metadata(testing_arrays, testing_labels)\n",
        "training_metadata = files_to_metadata(training_arrays, training_labels)"
      ],
      "metadata": {
        "id": "pEMaPWznUUxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def longest(data, labels, bound):\n",
        "  assert len(data) == len(labels)\n",
        "  restricted_data = []\n",
        "  restricted_labels = []\n",
        "  for i in range(len(data)):\n",
        "    if len(data[i]) > bound:\n",
        "      restricted_data.append(data[i])\n",
        "      restricted_labels.append(labels[i])\n",
        "  return restricted_data, restricted_labels"
      ],
      "metadata": {
        "id": "ONJp-tzws5y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_testing_arrays, longest_testing_labels = longest(testing_arrays, testing_labels, 44100)\n",
        "longest_training_arrays, longest_training_labels = longest(training_arrays, training_labels, 44100)"
      ],
      "metadata": {
        "id": "IFYPOpDCuOGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_training_metadata = files_to_metadata(longest_training_arrays, longest_training_labels)\n",
        "longest_testing_metadata = files_to_metadata(longest_testing_arrays, longest_testing_labels)\n",
        "longest_training_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q_pGAz4vFhf",
        "outputId": "93ce2a17-5ff5-45bf-8d4e-092fb434531c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'banjo': [53, 155097],\n",
              " 'bass-clarinet': [196, 95335],\n",
              " 'bassoon': [220, 109974],\n",
              " 'cello': [259, 82800],\n",
              " 'clarinet': [365, 100548],\n",
              " 'contrabassoon': [244, 108778],\n",
              " 'double-bass': [352, 76925],\n",
              " 'english-horn': [255, 138284],\n",
              " 'flute': [255, 96884],\n",
              " 'french-horn': [221, 115700],\n",
              " 'guitar': [53, 214066],\n",
              " 'mandolin': [26, 137388],\n",
              " 'oboe': [211, 73401],\n",
              " 'saxophone': [120, 122418],\n",
              " 'trombone': [178, 123409],\n",
              " 'trumpet': [214, 112213],\n",
              " 'tuba': [83, 128542],\n",
              " 'viola': [270, 67659],\n",
              " 'violin': [320, 66478]}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion: if we restrict to a dataset containing only files with at least 44100 samples, we still have decent statistics (>10 for all, and >100 for most). So we'll have a second of audio, and we'll select the second surrounding the peak of the sound.**"
      ],
      "metadata": {
        "id": "I1uDj_FCu2-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: I explicitly checked that the ordering of the audio files and labels was correct."
      ],
      "metadata": {
        "id": "3BpQrOgLUhco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing data"
      ],
      "metadata": {
        "id": "HNHY8H04TxFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def restricted_audio(audio, duration, rate):\n",
        "  points = rate * duration\n",
        "  middle = np.argmax(audio)\n",
        "  begin = int(middle-points/2)\n",
        "  end = int(middle+points/2)\n",
        "  if begin < 0:\n",
        "    begin = 0\n",
        "    end = int(points)\n",
        "  return audio[begin:end]"
      ],
      "metadata": {
        "id": "P5surNYFxuTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_data(data, duration, rate):\n",
        "  trimmed = []\n",
        "  for i in data:\n",
        "    trimmed.append(restricted_audio(i, duration, rate))\n",
        "  return trimmed"
      ],
      "metadata": {
        "id": "tgyiiX-02XU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_training_data = trim_data(longest_training_arrays, 1, 44100)\n",
        "short_training_data = trim_data(longest_training_arrays, 0.2, 44100)\n",
        "long_testing_data = trim_data(longest_testing_arrays, 1, 44100)\n",
        "short_testing_data = trim_data(longest_testing_arrays, 0.2, 44100)"
      ],
      "metadata": {
        "id": "TMNs0QkV2SBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now normalize data:\n",
        "def normalize(data):\n",
        "  return data - np.ones(len(data))*np.mean(data) / np.std(data)"
      ],
      "metadata": {
        "id": "GPmunZ525Ptm"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_long_norm = apply_to_set(normalize, long_training_data)\n",
        "training_short_norm = apply_to_set(normalize, short_training_data)\n",
        "testing_long_norm = apply_to_set(normalize, long_testing_data)\n",
        "testing_short_norm = apply_to_set(normalize, short_testing_data)"
      ],
      "metadata": {
        "id": "wIgtD19Y55ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEkQg4HN7TMv",
        "outputId": "81546fba-9caa-41f8-bfb0-2c541a853288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Songs',\n",
              " 'Instruments',\n",
              " 'train_instruments',\n",
              " '.ipynb_checkpoints',\n",
              " 'testing_instruments',\n",
              " 'Music.ipynb',\n",
              " 'test1.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('train_instruments/long_data_raw', training_long_norm, allow_pickle=True)\n",
        "np.save('train_instruments/short_data_raw', training_short_norm, allow_pickle=True)\n",
        "np.save('testing_instruments/long_data_raw', testing_long_norm, allow_pickle=True)\n",
        "np.save('testing_instruments/short_data_raw', testing_short_norm, allow_pickle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu8FKTRz7Qn2",
        "outputId": "6ef788e3-839e-42e4-c5e1-1e399ef6aff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('testing_instruments/final_labels', longest_testing_labels, allow_pickle=True)\n",
        "np.save('train_instruments/final_labels', longest_training_labels, allow_pickle=True)"
      ],
      "metadata": {
        "id": "iBs0gubu9q0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fourier transforms (and spectrograms?)"
      ],
      "metadata": {
        "id": "njau0V6X_PwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_long = np.load('train_instruments/long_data_raw.npy', allow_pickle=True)\n",
        "training_short = np.load('train_instruments/short_data_raw.npy', allow_pickle=True)\n",
        "testing_long = np.load('testing_instruments/long_data_raw.npy', allow_pickle=True)\n",
        "testing_short = np.load('testing_instruments/short_data_raw.npy', allow_pickle=True)\n",
        "\n",
        "training_labels = np.load('train_instruments/final_labels.npy', allow_pickle=True)\n",
        "testing_labels = np.load('testing_instruments/final_labels.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "8Z5SZZYmvUJ7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's turn one of the small clips into a Fourier mode. The decomposition should be for sines/cosines.\n",
        "# basic check we can do is that the leading frequency corresponds to the note correctly!"
      ],
      "metadata": {
        "id": "PuzQPGmvzUCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll pick the tenth guy. this is D note, which is 293.7 Hz, or 288.3 maybe.\n",
        "\n",
        "# https://pages.mtu.edu/~suits/notefreq432.html"
      ],
      "metadata": {
        "id": "sOpYXTit1QTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 44100"
      ],
      "metadata": {
        "id": "3HCn00SiBIVP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels[270]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7yEBj3et00gm",
        "outputId": "7fa99783-b715-4370-9966-eb4ec538c34b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'double-bass_As2_15_molto-pianissimo_arco-normal.mp3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(training_short[270])\n",
        "r = np.arange(0,len(x))\n",
        "y = scipy.fft.dct(x)\n",
        "plt.xlim(1, 500)\n",
        "plt.plot(r,y)\n",
        "#print(np.argmax(y))\n",
        "print(np.argmax(abs(y)) * rate /(2*len(x))) \n",
        "#print(590/288)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "3cKuTETA7xGe",
        "outputId": "b0ff8d29-7cd8-4d07-a41c-3bc110e32e88"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from looking at data above, it seems that the Fourier decomposition works relatively well, and correctly picks out the note in almost all cases.\n",
        "#NOTE: in some cases, for a reason I don't yet understand (e.g. in instrument 10 - violin_D4_1_piano_arco-normal.mp3) it picks out a frequency which is TWICE as large?\n",
        "#eventually try to run above analysis on entire dataset!"
      ],
      "metadata": {
        "id": "eEJnlnijnN88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_to_fourier(data):\n",
        "  x = np.array(data)\n",
        "  return scipy.fft.dct(x)\n",
        "\n",
        "fourier_training = apply_to_set(data_to_fourier, training_short)\n",
        "fourier_testing = apply_to_set(data_to_fourier, testing_short)"
      ],
      "metadata": {
        "id": "cOyjeZego8R8"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_zero_modes(data):\n",
        "  return np.delete(data, [0,1])\n",
        "\n",
        "fourier_training = apply_to_set(strip_zero_mode, fourier_training)\n",
        "fourier_testing = apply_to_set(strip_zero_mode, fourier_testing)"
      ],
      "metadata": {
        "id": "5Q-LWVVOp0Aj"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lastly, we need to normalize (or standardize) the data.\n",
        "fourier_training = apply_to_set(normalize, fourier_training)\n",
        "fourier_testing = apply_to_set(normalize, fourier_testing)"
      ],
      "metadata": {
        "id": "o1uIO1cRsM-y"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('train_instruments/fourier_training', fourier_training, allow_pickle=True)\n",
        "np.save('testing_instruments/fourier_testing', fourier_testing, allow_pickle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tY0NYrira8l",
        "outputId": "a526d2e9-4912-44ec-f2f4-25aae39cdd9b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_to_spectro(data):\n",
        "  return tfio.audio.spectrogram(data, nfft=512, window=512, stride=256)\n",
        "\n",
        "training_spectrograms = apply_to_set(raw_to_spectro, training_long)\n",
        "testing_spectrograms = apply_to_set(raw_to_spectro, testing_long)"
      ],
      "metadata": {
        "id": "sMiV-uosDklq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('train_instruments/training_spectrograms', training_spectrograms, allow_pickle=True)\n",
        "np.save('testing_instruments/testing_spectrograms', testing_spectrograms, allow_pickle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOQGWXgpE1fk",
        "outputId": "91774169-20d9-46f6-d39f-d2489f7644f5"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def file_to_instrument(file):\n",
        "  instrument = ''\n",
        "  for i in file:\n",
        "    if i == '_':\n",
        "      break\n",
        "    else:\n",
        "      instrument += i\n",
        "  return instrument\n",
        "\n",
        "training_labels_instruments_array = apply_to_set(file_to_instrument, training_labels)\n",
        "testing_labels_instruments_array = apply_to_set(file_to_instrument, testing_labels)\n",
        "\n",
        "training_labels_instruments = [str(i) for i in training_labels_instruments_array]\n",
        "testing_labels_instruments = [str(i) for i in testing_labels_instruments_array]\n",
        "\n",
        "np.save('train_instruments/instrument_labels_train', training_labels_instruments, allow_pickle=True)\n",
        "np.save('testing_instruments/instrument_labels_test', testing_labels_instruments, allow_pickle=True)"
      ],
      "metadata": {
        "id": "qju1iRhjM3-t"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the model"
      ],
      "metadata": {
        "id": "1qRy8YaKRVxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Normalize both training and testing data sets.\n",
        "\n",
        "6) Run analysis on: (maybe keep in mind to randomize order in which I feed data, e.g., not all violins in a row?)\n",
        "(a) Raw files\n",
        "(b) Fouriers\n",
        "(c) Spectrograms\n",
        "\n",
        "7) Try above with different architectures, experiment!\n",
        "\n",
        "8) See if I can come up with a different way to analyze the data\n",
        "\n",
        "9) Fingerprint of instruments? Generalize to other project if possible (of separating out instruments)\n",
        "\n",
        "10) ML experiments: see what happens as I change size of data set. How much data did I actually need? How does this vary as I change algorithms?"
      ],
      "metadata": {
        "id": "gAaV8gnMCT4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data"
      ],
      "metadata": {
        "id": "lP4UvHoJGh7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_long = np.load('train_instruments/long_data_raw.npy', allow_pickle=True)\n",
        "training_short = np.load('train_instruments/short_data_raw.npy', allow_pickle=True)\n",
        "training_fourier = np.load('train_instruments/fourier_training.npy', allow_pickle=True)\n",
        "training_spectro = np.load('train_instruments/training_spectrograms.npy', allow_pickle=True)\n",
        "\n",
        "testing_long = np.load('testing_instruments/long_data_raw.npy', allow_pickle=True)\n",
        "testing_short = np.load('testing_instruments/short_data_raw.npy', allow_pickle=True)\n",
        "testing_fourier = np.load('testing_instruments/fourier_testing.npy', allow_pickle=True)\n",
        "testing_spectro = np.load('testing_instruments/testing_spectrograms.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "train_labels_inst = np.load('train_instruments/instrument_labels_train.npy', allow_pickle=True)\n",
        "test_labels_inst = np.load('testing_instruments/instrument_labels_test.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "WeUbSlt_Gm4X"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keep in mind: spectrograms not normalized yet!"
      ],
      "metadata": {
        "id": "cC1u_CCuGSTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KywDj-u-RK-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#additionally: try to LEARN the notes with ML. Maybe the ML algorithm will perform better on weird things like file 10 at recognizing the note :). "
      ],
      "metadata": {
        "id": "bgTJwcfHDyEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with fourier, try to find all relevant (maybe like 5?) peaks and analyze them.\n",
        "#i'm guessing I can turn the 5-7 main peaks into an array and THAT data should also be enough for instruments? try that as a set of data as well!\n",
        "#see if ML can recognize the above point; i.e., how many effective nodes is it really using? try dropout, etc. to identify how many variables go into an instrument :).\n",
        "#for recognition it might be best to do small sample around biggest frequency? to pick up timbre with that. "
      ],
      "metadata": {
        "id": "ipJ7mMYvm2PK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4QPC9ZjDZ9XO",
        "PLGuCj4oaCxO",
        "0Zboon_yYGtY",
        "tfQUpgmDT8FU",
        "cEHxQvctSuJJ",
        "HNHY8H04TxFl",
        "njau0V6X_PwM"
      ],
      "name": "Music.ipynb",
      "provenance": [],
      "mount_file_id": "19GUYCoE594XnKrhdhHP9CYHGKCgRbduY",
      "authorship_tag": "ABX9TyOOHVRUrdgtmpd4eURhL6DO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}